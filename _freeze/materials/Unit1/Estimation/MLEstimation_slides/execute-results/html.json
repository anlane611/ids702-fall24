{
  "hash": "8593b21be0d74f6bdfcf927c4d77b7ea",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Maximum Likelihood Estimation\"\nformat: \n  revealjs:\n    incremental: true\n    multiplex: true\n    chalkboard: true\n    html-math-method: mathjax\nwebr: \n  show-startup-message: false\n  show-header-message: false\n  home-dir: '/home/r-user/'\n  packages: ['ggplot2', 'dplyr', 'FamilyRank']\nfilters:\n  - webr\nexecute:\n  freeze: auto\n---\n\n\n\n## Estimation Big Picture\n\n## Maximum Likelihood Estimation\n\nLet $X_1, X_2, ..., X_n \\stackrel{iid}{\\sim} F_{\\theta}$\n\n \n\nThen the **joint distribution** of ($X_1, ..., X_n$) is given by $f_{\\theta}(x_1,...,x_n)=\\prod_{i=1}^{n} f_{\\theta}(x_i)$\n\n \n\nNow, we define the **likelihood function** as a function of the fix population parameter $\\theta$ : $L(\\theta)=f_{\\theta}(X_1,...,X_n)=\\prod_{i=1}^nf_{\\theta}(X_i)$\n\nWe want to find the value of $\\theta$ that maximizes the likelihood function and call it $\\hat{\\theta}$\n\n## Illustration\n\nImagine we have a binary variable (e.g., disease status). n=100, where 30 have the disease.\n\nWe can use the Bernoulli distribution, so $\\prod_{i=1}^{n}p^{x_i} (1-p)^{1-x_i}$ and $L(p)=p^{30}(1-p)^{70}$ , which we can plot over a range of values of $p$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](MLEstimation_slides_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## General mechanics\n\n1.  Assume a distribution for the population\n2.  Define the likelihood: joint probability of the data\n3.  Take the log of the likelihood\n4.  Find the value of $\\theta$ that maximizes the likelihood:\n    1.  Differentiate the log-likelihood with respect to $\\theta$\n    2.  Set the derivative to 0\n    3.  Solve for $\\theta$\n\n## Evaluating Estimators\n\n-   Bias: Average distance from the population parameter\n\n    -   Bias($\\hat{\\theta}$) = $\\mathbf{E}[\\hat{\\theta}]-\\theta$\n\n-   Standard Error (SE): standard deviation of the estimator (i.e., SD of the sampling distribution)\n\n    -   STD($\\hat{\\theta}$)= $\\sqrt{\\mathbf{E}[\\hat{\\theta}-\\mathbf{E}(\\hat{\\theta})]^2}$\n\n-   Mean Squared Error (MSE): combines standard error and bias\n\n    -   MSE($\\hat{\\theta}$)= $\\mathbf{E}[\\hat{\\theta}-\\theta]^2$ = SE$^2$ + Bias$^2$\n\n## Example\n\nLet $X_1, X_2, ..., X_n \\stackrel{iid}{\\sim} Poisson(\\lambda)$\n\nDerive the MLE, $\\hat{\\lambda}$, and determine if this is an unbiased estimator of $\\lambda$\n\n$P(X=x)=\\frac{\\lambda^x  e^{-\\lambda}}{x !}$\n",
    "supporting": [
      "MLEstimation_slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}