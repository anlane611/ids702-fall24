{
  "hash": "e8c5e264f66ee4a10c0ea60e706ab857",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Maximum Likelihood Estimation\"\nformat: \n  revealjs:\n    incremental: true\n    multiplex: true\n    chalkboard: true\n    html-math-method: mathjax\nwebr: \n  show-startup-message: false\n  show-header-message: false\n  home-dir: '/home/r-user/'\n  packages: ['ggplot2', 'dplyr', 'FamilyRank']\nfilters:\n  - webr\nexecute:\n  freeze: auto\n---\n\n\n\n\n\n\n## Estimation Big Picture\n\n## Maximum Likelihood Estimation\n\nLet $X_1, X_2, ..., X_n \\stackrel{iid}{\\sim} F_{\\theta}$\n\n \n\nThen the **joint distribution** of ($X_1, ..., X_n$) is given by $f_{\\theta}(x_1,...,x_n)=\\prod_{i=1}^{n} f_{\\theta}(x_i)$\n\n \n\nNow, we define the **likelihood function** as a function of the fix population parameter $\\theta$ : $L(\\theta)=f_{\\theta}(X_1,...,X_n)=\\prod_{i=1}^nf_{\\theta}(X_i)$\n\nWe want to find the value of $\\theta$ that maximizes the likelihood function and call it $\\hat{\\theta}$\n\n## Illustration\n\nImagine we have a binary variable (e.g., disease status). n=100, where 30 have the disease.\n\nWe can use the Bernoulli distribution, so $\\prod_{i=1}^{n}p^{x_i} (1-p)^{1-x_i}$ and $L(p)=p^{30}(1-p)^{70}$ , which we can plot over a range of values of $p$\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](MLEstimation_slides_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\n## General mechanics\n\n1.  Assume a distribution for the population\n2.  Define the likelihood: joint probability of the data\n3.  Take the log of the likelihood\n4.  Find the value of $\\theta$ that maximizes the likelihood:\n    1.  Differentiate the log-likelihood with respect to $\\theta$\n    2.  Set the derivative to 0\n    3.  Solve for $\\theta$\n\n## Evaluating Estimators\n\n-   Bias: Average distance from the population parameter\n\n    -   Bias($\\hat{\\theta}$) = $\\mathbf{E}[\\hat{\\theta}]-\\theta$\n\n-   Standard Error (SE): standard deviation of the estimator (i.e., SD of the sampling distribution)\n\n    -   STD($\\hat{\\theta}$)= $\\sqrt{\\mathbf{E}[\\hat{\\theta}-\\mathbf{E}(\\hat{\\theta})]^2}$\n\n-   Mean Squared Error (MSE): combines standard error and bias\n\n    -   MSE($\\hat{\\theta}$)= $\\mathbf{E}[\\hat{\\theta}-\\theta]^2$ = SE$^2$ + Bias$^2$\n\n## Example\n\nLet $X_1, X_2, ..., X_n \\stackrel{iid}{\\sim} Poisson(\\lambda)$\n\nDerive the MLE, $\\hat{\\lambda}$, and determine if this is an unbiased estimator of $\\lambda$\n\n$P(X=x)=\\frac{\\lambda^x e^{-\\lambda}}{x !}$\n",
    "supporting": [
      "MLEstimation_slides_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}